from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
import shutil
import os
import time

# Global client reference to reuse
_chroma_client = None

def embed_document_for_indexing(processed_document, api_key, chunk_size=1000, chunk_overlap=200):
    """
    Chunk document and setup embedding model in one step.
    
    :param processed_document: The processed document text
    :param api_key: OpenAI API key
    :param chunk_size: Size of each chunk
    :param chunk_overlap: Overlap between chunks
    :return: ChromaDB collection
    """
    global _chroma_client
    
    # Remove existing ChromaDB directory
    chroma_path = "./chroma_user_docs"
    
    # Reset client if exists
    if _chroma_client is not None:
        try:
            del _chroma_client
            _chroma_client = None
            time.sleep(0.1)  # Small delay to ensure cleanup
        except:
            pass
    
    if os.path.exists(chroma_path):
        try:
            shutil.rmtree(chroma_path)
            print(f"Removed existing ChromaDB at {chroma_path}")
        except Exception as e:
            print(f"Warning: Could not remove {chroma_path}: {e}")
            # Try to force remove with different permissions
            import stat
            def remove_readonly(func, path, excinfo):
                os.chmod(path, stat.S_IWRITE)
                func(path)
            shutil.rmtree(chroma_path, onerror=remove_readonly)
            print(f"Force removed {chroma_path}")
    
    # Chunk the document
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    chunks = text_splitter.split_text(processed_document)
    print(f"Created {len(chunks)} chunks from document")
    
    # Setup embedding model
    embedding_function = embedding_functions.OpenAIEmbeddingFunction(
        api_key=api_key,
        model_name="text-embedding-3-small"
    )

    # Create fresh ChromaDB client
    _chroma_client = chromadb.PersistentClient(
        path=chroma_path,
        settings=Settings(
            allow_reset=True,
            anonymized_telemetry=False
        )
    )
    
    collection = _chroma_client.get_or_create_collection(
        name="chroma_user_docs",
        embedding_function=embedding_function
    )k: Number of relevant documents to retrieve
    :param collection_name: Name of the collection
    :return: Formatted context string
    """
    global _chroma_client
    
    if _chroma_client is None:
        chroma_path = "./chroma_user_docs"
        if not os.path.exists(chroma_path):
            return "No documents have been indexed yet. Please upload a document first."
        
        _chroma_client = chromadb.PersistentClient(
            path=chroma_path,
            settings=Settings(
                allow_reset=True,
                anonymized_telemetry=False
            )
        )
    
    try:
        vector_index = _chroma_client.get_collection(name=collection_name)
    except Exception as e:
        return f"Error accessing collection: {str(e)}"
    print(f"Successfully indexed {len(chunks)} chunks to ChromaDB")
    return collection

def query_documents(query, k=5,collection_name="chroma_user_docs"):
    """
    Retrieve and format context from the document index.
    
    :param query: The user's question
    :param vector_index: The ChromaDB index
    :param k: Number of relevant documents to retrieve
    :return: Formatted context string
    """
    client = chromadb.PersistentClient(path="./chroma_user_docs")
    vector_index = client.get_collection(
            name=collection_name
        )

    results = vector_index.query(
        query_texts=[query],
        n_results=k
    )
    
    context_chunks = [item for sublist in results['documents'] for item in sublist]
    formatted_context = "\n---\n".join(context_chunks)
    
    return formatted_context